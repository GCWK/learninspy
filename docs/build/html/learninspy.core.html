<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>learninspy.core package &mdash; documentación de Learninspy - 0.1.0</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="documentación de Learninspy - 0.1.0" href="index.html" />
    <link rel="next" title="learninspy.utils package" href="learninspy.utils.html" />
    <link rel="prev" title="learninspy.utils package" href="learninspy.utils.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navegación</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="learninspy.utils.html" title="learninspy.utils package"
             accesskey="N">siguiente</a></li>
        <li class="right" >
          <a href="learninspy.utils.html" title="learninspy.utils package"
             accesskey="P">anterior</a> |</li>
        <li><a href="index.html">documentación de Learninspy - 0.1.0</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="learninspy-core-package">
<h1>learninspy.core package<a class="headerlink" href="#learninspy-core-package" title="Enlazar permanentemente con este título">¶</a></h1>
<p>Éste es el módulo principal o el núcleo del framework, y contiene clases relacionadas con la construcción de redes neuronales profundas, desde el diseño de la arquitectura hasta la optimización del desempeño en las tareas asignadas.</p>
<div class="section" id="submodulos">
<h2>Submódulos<a class="headerlink" href="#submodulos" title="Enlazar permanentemente con este título">¶</a></h2>
</div>
<div class="section" id="learninspy-core-activations">
<h2>learninspy.core.activations<a class="headerlink" href="#learninspy-core-activations" title="Enlazar permanentemente con este título">¶</a></h2>
<p>En este módulo se pueden configurar las funciones de activación que se deseen. Para ello, simplemente se codifica tanto la función como su derivada analítica (o aproximación, como en el caso de la ReLU), y luego se insertan en los diccionarios de funciones correspondientes, que se encuentran al final del script, con una key común que identifique la activación.</p>
<span class="target" id="module-learninspy.core.activations"></span><dl class="function">
<dt id="learninspy.core.activations.identity">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">identity</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.identity" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Identidad</p>
<p><img class="math" src="_images/math/7139b967cb2b3a44e3425cc00ce13fe1e12c605d.png" alt="f(x)=x"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.identity_d">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">identity_d</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.identity_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de Identidad</p>
<p><img class="math" src="_images/math/1878988aecf7efce011aab2c6a7b943f60bb67ea.png" alt="f(x)=1"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.lecunn_sigmoid">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">lecunn_sigmoid</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.lecunn_sigmoid" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Sigmoidea recomendada por LeCunn</p>
<p><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-89.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-89.pdf</a></p>
<p><img class="math" src="_images/math/a3ca157b91dc9711ad5b6b3db00e2491db67cff4.png" alt="f(x)=1.7159 tanh(\dfrac{2x}{3})"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.lecunn_sigmoid_d">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">lecunn_sigmoid_d</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.lecunn_sigmoid_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de Sigmoidea recomendada por LeCunn</p>
<p><img class="math" src="_images/math/6ff81b9b1acf5e84d28e954540233d821ef9573e.png" alt="f(x)=1.14393 (1 - tanh^2(\dfrac{2x}{3}))"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.lrelu">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">lrelu</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.lrelu" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Leaky ReLU</p>
<p><img class="math" src="_images/math/67e458a82a3ee7c16839d0e593eeea9146dd6a1e.png" alt="f(x) = \begin{cases}x &amp; x &gt; 0 \\ 0.01x &amp; x \leq 0\end{cases}"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.lrelu_d">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">lrelu_d</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.lrelu_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de Leaky ReLU</p>
<p><img class="math" src="_images/math/2b08aba8fdacd533476ad8139e853c6cb5fbcf28.png" alt="f(x) = \begin{cases}1 &amp; x &gt; 0 \\ 0.01 &amp; x \leq 0\end{cases}"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.relu">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">relu</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.relu" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Rectifier Linear Unit (ReLU)</p>
<p><img class="math" src="_images/math/4b7aa4ce436c8a6d65b0504899474700f4bfb0e7.png" alt="f(x)=max(0,x)"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.relu_d">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">relu_d</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.relu_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de ReLU</p>
<p><img class="math" src="_images/math/ecad0c36059992464ad98d2c4b9d64fcbacf2a88.png" alt="f(x) = \begin{cases}1 &amp; x &gt; 0 \\ 0 &amp; x \leq 0\end{cases}"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.sigmoid">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">sigmoid</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.sigmoid" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Sigmoidea</p>
<p><img class="math" src="_images/math/fc778d278eb3cc690cf17e1a9bf8be73e5a776d0.png" alt="f(x)=\dfrac{1}{1 + e^{-x}}"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.sigmoid_d">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">sigmoid_d</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.sigmoid_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de Sigmoidea</p>
<p><img class="math" src="_images/math/cb77a5444af9578a12a2b1a9bb8d165f76eb8920.png" alt="f(x)=\dfrac{e^x - e^{-x}}{e^x + e^{-x}}"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.softplus">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">softplus</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.softplus" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Softplus</p>
<p><img class="math" src="_images/math/b0fcd3627b694e6af4488af3f1455e1fa908c772.png" alt="f(x)=\log{(1+e^x)}"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.softplus_d">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">softplus_d</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.softplus_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de Softplus</p>
<p><img class="math" src="_images/math/c22f8aaa311ba4e8e35eae34105d927f283a60ed.png" alt="f(x)=sigmoid(x)=\dfrac{1}{1 + e^{-x}}"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.tanh">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">tanh</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.tanh" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Tangente Hiperbolica</p>
<p><img class="math" src="_images/math/cb77a5444af9578a12a2b1a9bb8d165f76eb8920.png" alt="f(x)=\dfrac{e^x - e^{-x}}{e^x + e^{-x}}"/></p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.activations.tanh_d">
<tt class="descclassname">learninspy.core.activations.</tt><tt class="descname">tanh_d</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.activations.tanh_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de Tangente Hiperbolica</p>
<p><img class="math" src="_images/math/3a03243140e5a448b7f5c82471f166856b0bd7b5.png" alt="f(x)=1-tanh^2(x)"/></p>
</dd></dl>

</div>
<div class="section" id="module-learninspy.core.autoencoder">
<span id="learninspy-core-autoencoder"></span><h2>learninspy.core.autoencoder<a class="headerlink" href="#module-learninspy.core.autoencoder" title="Enlazar permanentemente con este título">¶</a></h2>
<dl class="class">
<dt id="learninspy.core.autoencoder.AutoEncoder">
<em class="property">class </em><tt class="descclassname">learninspy.core.autoencoder.</tt><tt class="descname">AutoEncoder</tt><big>(</big><em>params=None</em>, <em>list_layers=None</em>, <em>dropout_in=0.0</em><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.AutoEncoder" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <a class="reference internal" href="#learninspy.core.model.NeuralNetwork" title="learninspy.core.model.NeuralNetwork"><tt class="xref py py-class docutils literal"><span class="pre">learninspy.core.model.NeuralNetwork</span></tt></a></p>
<p>Tipo de red neuronal, compuesto de una capa de entrada, una oculta, y una de salida.
Las unidades en la capa de entrada y la de salida son iguales, y en la capa oculta
se entrena una representación de la entrada en distinta dimensión, mediante aprendizaje
no supervisado y backpropagation..
A las conexiones entre la capa de entrada y la oculta se le denominan <strong>encoder</strong>,
y a las de la oculta a la salida se les llama <strong>decoder</strong>.</p>
<p>Para más información, ver <a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity">http://ufldl.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> &#8211; model.NeuralNetworkParameters, donde se especifica la configuración de la red.</li>
<li><strong>list_layers</strong> &#8211; list de model.NeuralLayer, en caso de usar capas ya inicializadas.</li>
<li><strong>dropout_in</strong> &#8211; radio de DropOut usado para el encoder (el decoder no debe sufrir DropOut).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">ae_params</span> <span class="o">=</span> <span class="n">NetworkParameters</span><span class="p">(</span><span class="n">units_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;Tanh&#39;</span><span class="p">,</span> <span class="n">dropout_ratios</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">classification</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ae</span> <span class="o">=</span> <span class="n">AutoEncoder</span><span class="p">(</span><span class="n">ae_params</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="learninspy.core.autoencoder.AutoEncoder.assert_regression">
<tt class="descname">assert_regression</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.AutoEncoder.assert_regression" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Se asegura que el <em>decoder</em> corresponda a una capa de regresión
(que sea del tipo <em>model.RegressionLayer</em>).</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.autoencoder.AutoEncoder.encode">
<tt class="descname">encode</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.AutoEncoder.encode" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Codifica la entrada <strong>x</strong>, transformando los datos al pasarlos por el <em>encoder</em>.</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.autoencoder.AutoEncoder.encoder_layer">
<tt class="descname">encoder_layer</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.AutoEncoder.encoder_layer" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Devuelve la capa de <em>encoder</em>.</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.autoencoder.AutoEncoder.evaluate">
<tt class="descname">evaluate</tt><big>(</big><em>data</em>, <em>predictions=False</em><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.AutoEncoder.evaluate" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Evalúa AutoEncoder sobre un conjunto de datos.
Se utiliza <img class="math" src="_images/math/d954567732c83d1a3a8256c407193790859f4d1e.png" alt="r^2"/> como métrica en la evaluación.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>data</strong> &#8211; list de LabeledPoint</li>
<li><strong>predictions</strong> &#8211; si es True, retorna las predicciones (salida del AutoEncoder)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last">resultado de evaluación, y predicciones si se solicita en <em>predictions</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="learninspy.core.autoencoder.StackedAutoencoder">
<em class="property">class </em><tt class="descclassname">learninspy.core.autoencoder.</tt><tt class="descname">StackedAutoencoder</tt><big>(</big><em>params</em>, <em>list_layers=None</em>, <em>dropout=None</em><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.StackedAutoencoder" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <a class="reference internal" href="#learninspy.core.model.NeuralNetwork" title="learninspy.core.model.NeuralNetwork"><tt class="xref py py-class docutils literal"><span class="pre">learninspy.core.model.NeuralNetwork</span></tt></a></p>
<p>Estructura de red neuronal profunda, donde los pesos de cada capa son inicializados con los datos de entrenamiento
mediante <strong>autoencoders</strong>.</p>
<p>Para más información, ver <a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> &#8211; model.NeuralNetworkParameters, donde se especifica la configuración de la red.</li>
<li><strong>list_layers</strong> &#8211; list de model.NeuralLayer, en caso de usar capas ya inicializadas.</li>
<li><strong>dropout</strong> &#8211; radio de DropOut a utilizar en el <em>encoder</em> de cada <a class="reference internal" href="#learninspy.core.autoencoder.AutoEncoder" title="learninspy.core.autoencoder.AutoEncoder"><tt class="xref py py-class docutils literal"><span class="pre">AutoEncoder</span></tt></a>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="learninspy.core.autoencoder.StackedAutoencoder.finetune">
<tt class="descname">finetune</tt><big>(</big><em>train</em>, <em>valid</em>, <em>criterions=None</em>, <em>mini_batch=50</em>, <em>parallelism=4</em>, <em>optimizer_params=None</em>, <em>keep_best=False</em><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.StackedAutoencoder.finetune" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.autoencoder.StackedAutoencoder.fit">
<tt class="descname">fit</tt><big>(</big><em>train</em>, <em>valid=None</em>, <em>stops=None</em>, <em>mini_batch=50</em>, <em>parallelism=4</em>, <em>optimizer_params=None</em>, <em>keep_best=False</em><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.StackedAutoencoder.fit" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Fit de cada autoencoder usando conjuntos de entrenamiento y validación,
y su apilado para entrenar la red neuronal profunda con aprendizaje no supervisado.
Se especifica además cómo debe realizarse la optimización, mediante los parámetros explicados
en el método <a class="reference internal" href="#learninspy.core.model.NeuralNetwork.fit" title="learninspy.core.model.NeuralNetwork.fit"><tt class="xref py py-func docutils literal"><span class="pre">fit()</span></tt></a> de <a class="reference internal" href="#learninspy.core.model.NeuralNetwork" title="learninspy.core.model.NeuralNetwork"><tt class="xref py py-class docutils literal"><span class="pre">NeuralNetwork</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>train</strong> &#8211; </li>
<li><strong>valid</strong> &#8211; </li>
<li><strong>stops</strong> &#8211; </li>
<li><strong>mini_batch</strong> &#8211; </li>
<li><strong>parallelism</strong> &#8211; </li>
<li><strong>optimizer_params</strong> &#8211; </li>
<li><strong>keep_best</strong> &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.autoencoder.StackedAutoencoder.predict">
<tt class="descname">predict</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.autoencoder.StackedAutoencoder.predict" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="learninspy-core-loss">
<h2>learninspy.core.loss<a class="headerlink" href="#learninspy-core-loss" title="Enlazar permanentemente con este título">¶</a></h2>
<p>En este módulo se proveen dos funciones de costo populares, cuyo uso se corresponde a la tarea designada para el modelo:</p>
<ul class="simple">
<li><strong>Clasificación</strong>: Entropía Cruzada (en inglés, <em>Cross Entropy o CE</em>),</li>
<li><strong>Regresión</strong>: Error Cuadrático Medio (en inglés, <em>Mean Squared Error o MSE</em>).</li>
</ul>
<span class="target" id="module-learninspy.core.loss"></span><dl class="function">
<dt id="learninspy.core.loss.cross_entropy">
<tt class="descclassname">learninspy.core.loss.</tt><tt class="descname">cross_entropy</tt><big>(</big><em>o</em>, <em>t</em><big>)</big><a class="headerlink" href="#learninspy.core.loss.cross_entropy" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Función de entropía cruzada.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>o</strong> &#8211; list, correspondiente a la salida real.</li>
<li><strong>t</strong> &#8211; list, correspondiente a la salida esperada.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last">float, error de clasificación.</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Nota</p>
<p class="last">el vector <em>o</em> debe ser la salida de la función <a class="reference internal" href="#learninspy.core.neurons.LocalNeurons.softmax" title="learninspy.core.neurons.LocalNeurons.softmax"><tt class="xref py py-func docutils literal"><span class="pre">softmax()</span></tt></a></p>
</div>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.loss.cross_entropy_d">
<tt class="descclassname">learninspy.core.loss.</tt><tt class="descname">cross_entropy_d</tt><big>(</big><em>o</em>, <em>t</em><big>)</big><a class="headerlink" href="#learninspy.core.loss.cross_entropy_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de la función CE.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>o</strong> &#8211; list, correspondiente a la salida real.</li>
<li><strong>t</strong> &#8211; list, correspondiente a la salida esperada.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last">list, derivada de la función de error.</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Nota</p>
<p class="last">el vector <em>o</em> debe ser la salida de la función <a class="reference internal" href="#learninspy.core.neurons.LocalNeurons.softmax" title="learninspy.core.neurons.LocalNeurons.softmax"><tt class="xref py py-func docutils literal"><span class="pre">softmax()</span></tt></a></p>
</div>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.loss.mse">
<tt class="descclassname">learninspy.core.loss.</tt><tt class="descname">mse</tt><big>(</big><em>o</em>, <em>t</em><big>)</big><a class="headerlink" href="#learninspy.core.loss.mse" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Función de error cuadrático medio.
Ver más info en <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean squared error</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>o</strong> &#8211; list, correspondiente a la salida real.</li>
<li><strong>t</strong> &#8211; list, correspondiente a la salida esperada.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last">float, error de clasificación.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.loss.mse_d">
<tt class="descclassname">learninspy.core.loss.</tt><tt class="descname">mse_d</tt><big>(</big><em>o</em>, <em>t</em><big>)</big><a class="headerlink" href="#learninspy.core.loss.mse_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Derivada de la función MSE.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>o</strong> &#8211; list, correspondiente a la salida real.</li>
<li><strong>t</strong> &#8211; list, correspondiente a la salida esperada.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last">list, derivada de la función de error.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-learninspy.core.model">
<span id="learninspy-core-model"></span><h2>learninspy.core.model<a class="headerlink" href="#module-learninspy.core.model" title="Enlazar permanentemente con este título">¶</a></h2>
<dl class="class">
<dt id="learninspy.core.model.ClassificationLayer">
<em class="property">class </em><tt class="descclassname">learninspy.core.model.</tt><tt class="descname">ClassificationLayer</tt><big>(</big><em>n_in=2</em>, <em>n_out=2</em>, <em>activation='ReLU'</em>, <em>distributed=False</em>, <em>w=None</em>, <em>b=None</em>, <em>sparsity=False</em>, <em>rng=None</em><big>)</big><a class="headerlink" href="#learninspy.core.model.ClassificationLayer" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <a class="reference internal" href="#learninspy.core.model.NeuralLayer" title="learninspy.core.model.NeuralLayer"><tt class="xref py py-class docutils literal"><span class="pre">learninspy.core.model.NeuralLayer</span></tt></a></p>
<p>Clase correspondiente a la capa de salida en una red neuronal con tareas de clasificación.
Se distingue de una <a class="reference internal" href="#learninspy.core.model.RegressionLayer" title="learninspy.core.model.RegressionLayer"><tt class="xref py py-class docutils literal"><span class="pre">RegressionLayer</span></tt></a> en que para realizar la clasificación se define
que la activación se de por la función <em>softmax</em>.</p>
<dl class="method">
<dt id="learninspy.core.model.ClassificationLayer.dropoutput">
<tt class="descname">dropoutput</tt><big>(</big><em>x</em>, <em>p</em>, <em>grad=False</em><big>)</big><a class="headerlink" href="#learninspy.core.model.ClassificationLayer.dropoutput" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><div class="admonition warning">
<p class="first admonition-title">Advertencia</p>
<p class="last">No se debe aplicar Dropout en la capa de salida de una red neuronal,
por lo cual este método arroja un error de excepción.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.ClassificationLayer.output">
<tt class="descname">output</tt><big>(</big><em>x</em>, <em>grad=False</em><big>)</big><a class="headerlink" href="#learninspy.core.model.ClassificationLayer.output" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="learninspy.core.model.NetworkParameters">
<em class="property">class </em><tt class="descclassname">learninspy.core.model.</tt><tt class="descname">NetworkParameters</tt><big>(</big><em>units_layers</em>, <em>activation='ReLU'</em>, <em>layer_distributed=None</em>, <em>dropout_ratios=None</em>, <em>classification=True</em>, <em>strength_l1=1e-05</em>, <em>strength_l2=0.0001</em>, <em>seed=123</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NetworkParameters" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clase utilizada para especificar todos los parámetros necesarios para configurar una red neuronal</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first last simple">
<li><strong>units_layers</strong> &#8211; list of ints, donde cada valor indica la cantidad de unidades
que posee la respectiva capa. La cantidad de valores de la lista indica el total
de capas que va a tener la red (entrada + ocultas + salida).</li>
<li><strong>activation</strong> &#8211; string or list of strings, indicando la key de la/s activación/es a utilizar en
las capas de la red neuronal.</li>
<li><strong>layer_distributed</strong> &#8211; list of bools, indicando por cada capa si sus neuronas van a representarse
o no por arreglos distribuidos (<strong>no tiene efecto en este release</strong>).</li>
<li><strong>dropout_ratios</strong> &#8211; list of floats, indicando el valor de <em>p</em> para aplicar Dropout en cada
respectiva capa.</li>
<li><strong>classification</strong> &#8211; bool, es <em>True</em> si la tarea de la red es de clasificación y <em>False</em>
si es de regresión.</li>
<li><strong>strength_l1</strong> &#8211; float, ratio de Norma <strong>L1</strong> a aplicar en todas las capas.</li>
<li><strong>strength_l2</strong> &#8211; float, ratio de Norma <strong>L2</strong> a aplicar en todas las capas.</li>
<li><strong>seed</strong> &#8211; int, semilla que alimenta al generador de números aleatorios <strong>numpy.random.RandomState</strong>
utilizado por la red.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="learninspy.core.model.NeuralLayer">
<em class="property">class </em><tt class="descclassname">learninspy.core.model.</tt><tt class="descname">NeuralLayer</tt><big>(</big><em>n_in=2</em>, <em>n_out=2</em>, <em>activation='ReLU'</em>, <em>distributed=False</em>, <em>w=None</em>, <em>b=None</em>, <em>sparsity=False</em>, <em>rng=None</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralLayer" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Clase básica para modelar una capa de neuronas que compone una red neuronal.
Contiene sus &#8220;neuronas&#8221; representadas por pesos sinápticos <strong>w</strong> y <strong>b</strong>,
además de una función de activación asociada para dichos pesos.</p>
<p>Una correcta inicialización de los pesos sinápticos está muy ligada a la función de activación elegida.
Por defecto, los pesos sinápticos se inicializan con una distribución uniforme
con media <img class="math" src="_images/math/74c081db590f3d35421c1f6b9afd4cdda36ee210.png" alt="0"/> y varianza <img class="math" src="_images/math/eae8ba29dde14938e5de71b15b0de92bb7fb4390.png" alt="\frac{2.0}{\sqrt{n_{in}}}"/>,
lo cual da buenos resultados especialmente usando ReLUs.
Para la función <em>Tanh</em> se muestrea sobre una distribución uniforme
en el rango <img class="math" src="_images/math/cf6962fa2fc91b2c69c93823bfabe8874bf93679.png" alt="\pm \sqrt{\frac{6}{n_{in}+n_{out}}}"/>, y para la <em>Sigmoid</em> en el rango
<img class="math" src="_images/math/218ec60143ea1c89251e8d55f767c406433d25dd.png" alt="\pm 4.0 \sqrt{\frac{6}{n_{in}+n_{out}}}"/>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_in</strong> &#8211; int, dimensión de la entrada.</li>
<li><strong>n_out</strong> &#8211; int, dimensión de la salida.</li>
<li><strong>activation</strong> &#8211; string, key de la función de activación asignada a la capa.</li>
<li><strong>distribute</strong> &#8211; si es True, indica que se utilicen arreglos distribuidos para <strong>w</strong> y <strong>b</strong>.</li>
<li><strong>w</strong> &#8211; <a class="reference internal" href="#learninspy.core.neurons.LocalNeurons" title="learninspy.core.neurons.LocalNeurons"><tt class="xref py py-class docutils literal"><span class="pre">LocalNeurons</span></tt></a>, matriz de pesos sinápticos.</li>
<li><strong>b</strong> &#8211; <a class="reference internal" href="#learninspy.core.neurons.LocalNeurons" title="learninspy.core.neurons.LocalNeurons"><tt class="xref py py-class docutils literal"><span class="pre">LocalNeurons</span></tt></a>, vector de pesos bias.</li>
<li><strong>sparsity</strong> &#8211; si es True, los arreglos se almacenan en formato <strong>scipy.sparse.csr_matrix</strong>.</li>
<li><strong>rng</strong> &#8211; si es <em>None</em>, se crea un generador de números aleatorios mediante una instancia <strong>numpy.random.RandomState</strong>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Nota</p>
<p class="last">el parámetro <em>distribute</em> no tiene efecto, ya que el uso de arreglos distribuidos se deja para un próximo release.</p>
</div>
<dl class="method">
<dt id="learninspy.core.model.NeuralLayer.dropoutput">
<tt class="descname">dropoutput</tt><big>(</big><em>x</em>, <em>p</em>, <em>grad=False</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralLayer.dropoutput" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Salida de la capa neuronal, luego de aplicar la regularización de los pesos sinápticos por Dropout.</p>
<p><a class="reference external" href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> &#8211; <strong>numpy.array</strong>, vector de entrada</li>
<li><strong>p</strong> &#8211; float, tal que <img class="math" src="_images/math/eaefb689569d36bc921fe09ae34e70055519ae2c.png" alt="0&lt;p&lt;1"/></li>
<li><strong>grad</strong> &#8211; Si es <em>True</em>, se retorna además el gradiente de la salida.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last"><strong>numpy.array</strong>, o tupla de ellos si <em>grad</em> es True.</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Nota</p>
<p class="last">En las predicciones de la red no se debe efectuar Dropout.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralLayer.get_bias">
<tt class="descname">get_bias</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralLayer.get_bias" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Se devuelve el vector de bias <strong>b</strong>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Devuelve:</th><td class="field-body"><strong>numpy.array</strong>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralLayer.get_weights">
<tt class="descname">get_weights</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralLayer.get_weights" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Se devuelve la matriz de pesos sinápticos <strong>w</strong>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Devuelve:</th><td class="field-body"><strong>numpy.array</strong>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralLayer.l1">
<tt class="descname">l1</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralLayer.l1" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Norma <strong>L1</strong> sobre la matriz <strong>w</strong> de pesos sinápticos.
:return: float, resultado de aplicar norma.</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralLayer.l2">
<tt class="descname">l2</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralLayer.l2" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Norma <strong>L2</strong> sobre la matriz <strong>w</strong> de pesos sinápticos.
:return: float, resultado de aplicar norma.</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralLayer.output">
<tt class="descname">output</tt><big>(</big><em>x</em>, <em>grad=False</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralLayer.output" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Salida de la capa neuronal. Se toma una entrada <img class="math" src="_images/math/33de986fd5591527713105de21686523b7779b1a.png" alt="x \in \Re^{n_{in}}"/>, se pondera con los
pesos sinápticos <strong>W</strong> y el bias <strong>b</strong>, y luego se aplica la función de activación <strong>f</strong> para retornar el
resultado <img class="math" src="_images/math/e9f408ffd2770fe4415705306b867f8de0515a2b.png" alt="a = f(Wx + b)"/>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> &#8211; <strong>numpy.array</strong>, vector de entrada</li>
<li><strong>grad</strong> &#8211; Si es <em>True</em>, se retorna además el gradiente de la salida.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last"><strong>numpy.array</strong>, o tupla de ellos si <em>grad</em> es True.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralLayer.update">
<tt class="descname">update</tt><big>(</big><em>step_w</em>, <em>step_b</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralLayer.update" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Se actualizan los arreglos <strong>w</strong> y <strong>b</strong> sumando respectivamente los incrementos
recibidos por parámetros.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first last simple">
<li><strong>step_w</strong> &#8211; <a class="reference internal" href="#learninspy.core.neurons.LocalNeurons" title="learninspy.core.neurons.LocalNeurons"><tt class="xref py py-class docutils literal"><span class="pre">LocalNeurons</span></tt></a></li>
<li><strong>step_b</strong> &#8211; <a class="reference internal" href="#learninspy.core.neurons.LocalNeurons" title="learninspy.core.neurons.LocalNeurons"><tt class="xref py py-class docutils literal"><span class="pre">LocalNeurons</span></tt></a></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="learninspy.core.model.NeuralNetwork">
<em class="property">class </em><tt class="descclassname">learninspy.core.model.</tt><tt class="descname">NeuralNetwork</tt><big>(</big><em>params</em>, <em>list_layers=None</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clase para modelar una red neuronal. La misma soporta funcionalidades para configuración y diseño,
y para la optimización y testeo sobre un conjunto de datos cargado. Además ofrece funciones para
cargar y guardar un modelo entrenado.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>params</strong> &#8211; <a class="reference internal" href="#learninspy.core.model.NetworkParameters" title="learninspy.core.model.NetworkParameters"><tt class="xref py py-class docutils literal"><span class="pre">NetworkParameters</span></tt></a>, parámetros que configuran la red.</li>
<li><strong>list_layers</strong> &#8211; list of <tt class="xref py py-class docutils literal"><span class="pre">NruralLayer</span></tt>, en caso de que se utilicen capas de neuronas
ya creadas.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.check_gradients">
<tt class="descname">check_gradients</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.check_gradients" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.check_stop">
<tt class="descname">check_stop</tt><big>(</big><em>epochs</em>, <em>criterions</em>, <em>check_all=False</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.check_stop" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.cost">
<tt class="descname">cost</tt><big>(</big><em>features</em>, <em>label</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.cost" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.evaluate">
<tt class="descname">evaluate</tt><big>(</big><em>data</em>, <em>predictions=False</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.evaluate" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>data</strong> &#8211; </li>
<li><strong>predictions</strong> &#8211; bool, for returning predictions too</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.fit">
<tt class="descname">fit</tt><big>(</big><em>train</em>, <em>valid=None</em>, <em>stops=None</em>, <em>mini_batch=50</em>, <em>parallelism=4</em>, <em>optimizer_params=None</em>, <em>keep_best=False</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.fit" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.l1">
<tt class="descname">l1</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.l1" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.l2">
<tt class="descname">l2</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.l2" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.load">
<tt class="descname">load</tt><big>(</big><em>name</em>, <em>path</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.load" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.persist_layers">
<tt class="descname">persist_layers</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.persist_layers" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.predict">
<tt class="descname">predict</tt><big>(</big><em>x</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.predict" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.save">
<tt class="descname">save</tt><big>(</big><em>name</em>, <em>path</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.save" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.train">
<tt class="descname">train</tt><big>(</big><em>train_bc</em>, <em>mini_batch=50</em>, <em>parallelism=4</em>, <em>optimizer_params=None</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.train" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>train_bc</strong> &#8211; </li>
<li><strong>mini_batch</strong> &#8211; </li>
<li><strong>parallelism</strong> &#8211; </li>
<li><strong>optimizer_params</strong> &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.unpersist_layers">
<tt class="descname">unpersist_layers</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.unpersist_layers" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.model.NeuralNetwork.update">
<tt class="descname">update</tt><big>(</big><em>stepw</em>, <em>stepb</em><big>)</big><a class="headerlink" href="#learninspy.core.model.NeuralNetwork.update" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="learninspy.core.model.RegressionLayer">
<em class="property">class </em><tt class="descclassname">learninspy.core.model.</tt><tt class="descname">RegressionLayer</tt><big>(</big><em>n_in=2</em>, <em>n_out=2</em>, <em>activation='ReLU'</em>, <em>distributed=False</em>, <em>w=None</em>, <em>b=None</em>, <em>sparsity=False</em>, <em>rng=None</em><big>)</big><a class="headerlink" href="#learninspy.core.model.RegressionLayer" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <a class="reference internal" href="#learninspy.core.model.NeuralLayer" title="learninspy.core.model.NeuralLayer"><tt class="xref py py-class docutils literal"><span class="pre">learninspy.core.model.NeuralLayer</span></tt></a></p>
<p>Clase correspondiente a la capa de salida en una red neuronal con tareas de regresión,
utilizando la función de activación como salida de la red.</p>
<div class="admonition note">
<p class="first admonition-title">Nota</p>
<p class="last">No es recomendado utilizar Dropout en las capas de una red neuronal con tareas de regresión.</p>
</div>
<dl class="method">
<dt id="learninspy.core.model.RegressionLayer.dropoutput">
<tt class="descname">dropoutput</tt><big>(</big><em>x</em>, <em>p</em>, <em>grad=False</em><big>)</big><a class="headerlink" href="#learninspy.core.model.RegressionLayer.dropoutput" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><div class="admonition warning">
<p class="first admonition-title">Advertencia</p>
<p class="last">No se debe aplicar Dropout en la capa de salida de una red neuronal,
por lo cual este método arroja un error de excepción.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-learninspy.core.neurons">
<span id="learninspy-core-neurons"></span><h2>learninspy.core.neurons<a class="headerlink" href="#module-learninspy.core.neurons" title="Enlazar permanentemente con este título">¶</a></h2>
<dl class="class">
<dt id="learninspy.core.neurons.LocalNeurons">
<em class="property">class </em><tt class="descclassname">learninspy.core.neurons.</tt><tt class="descname">LocalNeurons</tt><big>(</big><em>mat</em>, <em>shape</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Clase principal para representar los pesos sinápticos <strong>W</strong> de una red neuronal,
y el bias <strong>b</strong>. Provee funcionalidades algebraicas para operar matrices y vectores,
así como también normas regularizadoras y la aplicación de funciónes de activación.
No obstante, esta clase es usada directamente por <a class="reference internal" href="#learninspy.core.model.NeuralLayer" title="learninspy.core.model.NeuralLayer"><tt class="xref py py-class docutils literal"><span class="pre">NeuralLayer</span></tt></a>, por lo cual
no es herramienta de libre utilidad.</p>
<div class="admonition note">
<p class="first admonition-title">Nota</p>
<p class="last">Es preciso aclarar que su estructuración se debe a que está pensada para ser compatible con su par <em>DistributedNeurons</em>, pero que en esta versión se encuentra inhabilitada.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first last simple">
<li><strong>mat</strong> &#8211; numpy.array o list o pyspark.rdd.PipelinedRDD, que corresponde a la matriz <strong>W</strong>
o vector <strong>b</strong> a alojar y operar.</li>
<li><strong>shape</strong> &#8211; tuple, que corresponde a la dimensión que debe tener <em>mat</em>. Útil sólo cuando se
utilizan arreglos distribuidos.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">high</span><span class="o">=+</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="nb">float</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">LocalNeurons</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.activation">
<tt class="descname">activation</tt><big>(</big><em>fun</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.activation" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Aplica una función de activación sobre cada entrada del arreglo alojado.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><strong>fun</strong> &#8211; función soportada en <a class="reference internal" href="#module-learninspy.core.activations" title="learninspy.core.activations"><tt class="xref py py-mod docutils literal"><span class="pre">activations</span></tt></a></td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.collect">
<tt class="descname">collect</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.collect" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Retorna el arreglo alojado.
:return: np.array</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.count">
<tt class="descname">count</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.count" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Cantidad de elementos de la matriz almacenada. Siendo MxN las dimensiones, retorna el producto de ambas.
:return: int</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.dot">
<tt class="descname">dot</tt><big>(</big><em>vec</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.dot" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Producto punto entre vectores. Equivalente a <em>numpy.array.dot</em>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><strong>vec</strong> &#8211; np.array o list.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.dropout">
<tt class="descname">dropout</tt><big>(</big><em>p</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.dropout" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Aplica DropOut <a class="reference internal" href="#srivastava2014dropout" id="id1">[srivastava2014dropout]</a> sobre el vector alojado, anulando sus elementos con una probabilidad <em>p</em>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><strong>p</strong> &#8211; float, tal que <img class="math" src="_images/math/eaefb689569d36bc921fe09ae34e70055519ae2c.png" alt="0&lt;p&lt;1"/></td>
</tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="srivastava2014dropout" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[srivastava2014dropout]</a></td><td>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014):
&#8220;Dropout: A simple way to prevent neural networks from overfitting&#8221;.
The Journal of Machine Learning Research, 15(1), 1929-1958.</td></tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.l1">
<tt class="descname">l1</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.l1" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Norma <strong>L1</strong> sobre la matriz almacenada. Se retorna una tupla con el resultado y además el gradiente de
dicha norma.
:return: tuple de float y LocalNeurons</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.l2">
<tt class="descname">l2</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.l2" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Norma <strong>L1</strong> sobre la matriz almacenada. Se retorna una tupla con el resultado y además el gradiente de
dicha norma.
:return: tuple de float y LocalNeurons</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.loss">
<tt class="descname">loss</tt><big>(</big><em>fun</em>, <em>y</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.loss" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Aplica una función de error entre el vector almacenado y el vector <em>y</em>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>fun</strong> &#8211; función soportada en <a class="reference internal" href="#module-learninspy.core.loss" title="learninspy.core.loss"><tt class="xref py py-mod docutils literal"><span class="pre">loss</span></tt></a></li>
<li><strong>y</strong> &#8211; list o np.array</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last">float</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.loss_d">
<tt class="descname">loss_d</tt><big>(</big><em>fun</em>, <em>y</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.loss_d" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Aplica una función derivada de error entre el vector almacenado y el vector <em>y</em>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><ul class="first simple">
<li><strong>fun</strong> &#8211; función derivada soportada en <a class="reference internal" href="#module-learninspy.core.loss" title="learninspy.core.loss"><tt class="xref py py-mod docutils literal"><span class="pre">loss</span></tt></a></li>
<li><strong>y</strong> &#8211; list o np.array</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Devuelve:</th><td class="field-body"><p class="first last">LocalNeurons</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.mul_array">
<tt class="descname">mul_array</tt><big>(</big><em>array</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.mul_array" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.mul_elemwise">
<tt class="descname">mul_elemwise</tt><big>(</big><em>array</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.mul_elemwise" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Producto elemento a elemento con <em>array</em>. Equivalente a llamar a <em>numpy.multiply</em> de dos arreglos.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><strong>array</strong> &#8211; LocalNeurons o np.array, de la misma dimensión que el arreglo alojado en la instancia.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.outer">
<tt class="descname">outer</tt><big>(</big><em>array</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.outer" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="learninspy.core.neurons.LocalNeurons.shape">
<tt class="descname">shape</tt><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.shape" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Dimensiones del arreglo alojado.
:return: tuple</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.softmax">
<tt class="descname">softmax</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.softmax" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Aplica la función <em>Softmax</em> sobre el vector alojado.</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.sum">
<tt class="descname">sum</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.sum" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Suma de todos los elementos del arreglo alojado.</p>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.sum_array">
<tt class="descname">sum_array</tt><big>(</big><em>array</em><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.sum_array" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Suma elemento a elemento con <em>array</em>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parámetros:</th><td class="field-body"><strong>array</strong> &#8211; LocalNeurons o np.array, de la misma dimensión que el arreglo alojado en la instancia.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="learninspy.core.neurons.LocalNeurons.transpose">
<tt class="descname">transpose</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.neurons.LocalNeurons.transpose" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Transpone el arreglo alojado en la instancia. Equivale a <em>numpy.array.transpose()</em>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="learninspy-core-optimization">
<h2>learninspy.core.optimization<a class="headerlink" href="#learninspy-core-optimization" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Este módulo se realizó en base al excelente package de optimización <a class="reference external" href="https://github.com/BRML/climin">climin</a> , de donde se adaptaron algunos algoritmos de optimización para su uso en redes neuronales.</p>
<div class="admonition note">
<p class="first admonition-title">Nota</p>
<p class="last">Proximamente se migrará a un package <em>optimization</em>, separando por scripts los algoritmos de optimización.</p>
</div>
<span class="target" id="module-learninspy.core.optimization"></span><dl class="class">
<dt id="learninspy.core.optimization.Adadelta">
<em class="property">class </em><tt class="descclassname">learninspy.core.optimization.</tt><tt class="descname">Adadelta</tt><big>(</big><em>model</em>, <em>data</em>, <em>parameters=None</em><big>)</big><a class="headerlink" href="#learninspy.core.optimization.Adadelta" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <a class="reference internal" href="#learninspy.core.optimization.Optimizer" title="learninspy.core.optimization.Optimizer"><tt class="xref py py-class docutils literal"><span class="pre">learninspy.core.optimization.Optimizer</span></tt></a></p>
</dd></dl>

<dl class="class">
<dt id="learninspy.core.optimization.GD">
<em class="property">class </em><tt class="descclassname">learninspy.core.optimization.</tt><tt class="descname">GD</tt><big>(</big><em>model</em>, <em>data</em>, <em>parameters=None</em><big>)</big><a class="headerlink" href="#learninspy.core.optimization.GD" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <a class="reference internal" href="#learninspy.core.optimization.Optimizer" title="learninspy.core.optimization.Optimizer"><tt class="xref py py-class docutils literal"><span class="pre">learninspy.core.optimization.Optimizer</span></tt></a></p>
</dd></dl>

<dl class="class">
<dt id="learninspy.core.optimization.Optimizer">
<em class="property">class </em><tt class="descclassname">learninspy.core.optimization.</tt><tt class="descname">Optimizer</tt><big>(</big><em>model</em>, <em>data</em>, <em>parameters=None</em><big>)</big><a class="headerlink" href="#learninspy.core.optimization.Optimizer" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<dl class="method">
<dt id="learninspy.core.optimization.Optimizer.check_stop">
<tt class="descname">check_stop</tt><big>(</big><em>check_all=False</em><big>)</big><a class="headerlink" href="#learninspy.core.optimization.Optimizer.check_stop" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="learninspy.core.optimization.Optimizer.results">
<tt class="descname">results</tt><big>(</big><big>)</big><a class="headerlink" href="#learninspy.core.optimization.Optimizer.results" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="learninspy.core.optimization.OptimizerParameters">
<em class="property">class </em><tt class="descclassname">learninspy.core.optimization.</tt><tt class="descname">OptimizerParameters</tt><big>(</big><em>algorithm='Adadelta'</em>, <em>options=None</em>, <em>stops=None</em>, <em>merge_criter='w_avg'</em>, <em>merge_goal='hits'</em><big>)</big><a class="headerlink" href="#learninspy.core.optimization.OptimizerParameters" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="learninspy.core.optimization.merge_models">
<tt class="descclassname">learninspy.core.optimization.</tt><tt class="descname">merge_models</tt><big>(</big><em>results_rdd</em>, <em>criter='w_avg'</em>, <em>goal='hits'</em><big>)</big><a class="headerlink" href="#learninspy.core.optimization.merge_models" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Funcion para hacer merge de modelos, en base a un criterio de ponderacion sobre un valor objetivo
:param results_rdd:
:param criter:
:param goal:
:return:</p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.optimization.mix_models">
<tt class="descclassname">learninspy.core.optimization.</tt><tt class="descname">mix_models</tt><big>(</big><em>left</em>, <em>right</em><big>)</big><a class="headerlink" href="#learninspy.core.optimization.mix_models" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Se devuelve el resultado de sumar las NeuralLayers
de left y right
:param left: list of NeuralLayer
:param right: list of NeuralLayer
:return: list of NeuralLayer</p>
</dd></dl>

<dl class="function">
<dt id="learninspy.core.optimization.optimize">
<tt class="descclassname">learninspy.core.optimization.</tt><tt class="descname">optimize</tt><big>(</big><em>model</em>, <em>data</em>, <em>params=None</em>, <em>mini_batch=50</em>, <em>seed=123</em><big>)</big><a class="headerlink" href="#learninspy.core.optimization.optimize" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-learninspy.core.search">
<span id="learninspy-core-search"></span><h2>learninspy.core.search<a class="headerlink" href="#module-learninspy.core.search" title="Enlazar permanentemente con este título">¶</a></h2>
<dl class="class">
<dt id="learninspy.core.search.RandomSearch">
<em class="property">class </em><tt class="descclassname">learninspy.core.search.</tt><tt class="descname">RandomSearch</tt><big>(</big><em>net_params</em>, <em>n_layers=0</em>, <em>n_iter=100</em>, <em>net_domain=None</em>, <em>seed=None</em><big>)</big><a class="headerlink" href="#learninspy.core.search.RandomSearch" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<dl class="method">
<dt id="learninspy.core.search.RandomSearch.fit">
<tt class="descname">fit</tt><big>(</big><em>train</em>, <em>valid</em>, <em>test</em>, <em>mini_batch=100</em>, <em>parallelism=4</em>, <em>stops=None</em>, <em>optimizer_params=None</em>, <em>keep_best=True</em><big>)</big><a class="headerlink" href="#learninspy.core.search.RandomSearch.fit" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

<dl class="staticmethod">
<dt id="learninspy.core.search.RandomSearch.guardar_config">
<em class="property">static </em><tt class="descname">guardar_config</tt><big>(</big><em>caballo</em>, <em>metrica</em>, <em>n_iter</em><big>)</big><a class="headerlink" href="#learninspy.core.search.RandomSearch.guardar_config" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-learninspy.core.stops">
<span id="learninspy-core-stops"></span><h2>learninspy.core.stops<a class="headerlink" href="#module-learninspy.core.stops" title="Enlazar permanentemente con este título">¶</a></h2>
<dl class="class">
<dt id="learninspy.core.stops.AchieveTolerance">
<em class="property">class </em><tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">AchieveTolerance</tt><big>(</big><em>tolerance</em>, <em>key='hits'</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.AchieveTolerance" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
</dd></dl>

<dl class="class">
<dt id="learninspy.core.stops.MaxIterations">
<em class="property">class </em><tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">MaxIterations</tt><big>(</big><em>max_iter</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.MaxIterations" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
</dd></dl>

<dl class="class">
<dt id="learninspy.core.stops.ModuloNIterations">
<em class="property">class </em><tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">ModuloNIterations</tt><big>(</big><em>n</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.ModuloNIterations" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
</dd></dl>

<dl class="class">
<dt id="learninspy.core.stops.NotBetterThanAfter">
<em class="property">class </em><tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">NotBetterThanAfter</tt><big>(</big><em>minimal</em>, <em>after</em>, <em>key='hits'</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.NotBetterThanAfter" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
</dd></dl>

<dl class="attribute">
<dt id="learninspy.core.stops.OnSignal">
<tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">OnSignal</tt><a class="headerlink" href="#learninspy.core.stops.OnSignal" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>alias de <a class="reference internal" href="#learninspy.core.stops.OnUnixSignal" title="learninspy.core.stops.OnUnixSignal"><tt class="xref py py-class docutils literal"><span class="pre">OnUnixSignal</span></tt></a></p>
</dd></dl>

<dl class="class">
<dt id="learninspy.core.stops.OnUnixSignal">
<em class="property">class </em><tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">OnUnixSignal</tt><big>(</big><em>sig=2</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.OnUnixSignal" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Stopping criterion that is sensitive to some signal.</p>
<dl class="method">
<dt id="learninspy.core.stops.OnUnixSignal.handler">
<tt class="descname">handler</tt><big>(</big><em>signal</em>, <em>frame</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.OnUnixSignal.handler" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="learninspy.core.stops.OnWindowsSignal">
<em class="property">class </em><tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">OnWindowsSignal</tt><big>(</big><em>sig=None</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.OnWindowsSignal" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Stopping criterion that is sensitive to signals Ctrl-C or Ctrl-Break
on Windows.</p>
<dl class="method">
<dt id="learninspy.core.stops.OnWindowsSignal.handler">
<tt class="descname">handler</tt><big>(</big><em>ctrl_type</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.OnWindowsSignal.handler" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="learninspy.core.stops.Patience">
<em class="property">class </em><tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">Patience</tt><big>(</big><em>initial</em>, <em>key='hits'</em>, <em>grow_factor=1.0</em>, <em>grow_offset=0.0</em>, <em>threshold=0.05</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.Patience" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
</dd></dl>

<dl class="class">
<dt id="learninspy.core.stops.TimeElapsed">
<em class="property">class </em><tt class="descclassname">learninspy.core.stops.</tt><tt class="descname">TimeElapsed</tt><big>(</big><em>sec</em><big>)</big><a class="headerlink" href="#learninspy.core.stops.TimeElapsed" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clases base: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/Learninspy-logo_grande2.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Tabla de Contenidos</a></h3>
  <ul>
<li><a class="reference internal" href="#">learninspy.core package</a><ul>
<li><a class="reference internal" href="#submodulos">Submódulos</a></li>
<li><a class="reference internal" href="#learninspy-core-activations">learninspy.core.activations</a></li>
<li><a class="reference internal" href="#module-learninspy.core.autoencoder">learninspy.core.autoencoder</a></li>
<li><a class="reference internal" href="#learninspy-core-loss">learninspy.core.loss</a></li>
<li><a class="reference internal" href="#module-learninspy.core.model">learninspy.core.model</a></li>
<li><a class="reference internal" href="#module-learninspy.core.neurons">learninspy.core.neurons</a></li>
<li><a class="reference internal" href="#learninspy-core-optimization">learninspy.core.optimization</a></li>
<li><a class="reference internal" href="#module-learninspy.core.search">learninspy.core.search</a></li>
<li><a class="reference internal" href="#module-learninspy.core.stops">learninspy.core.stops</a></li>
</ul>
</li>
</ul>

  <h4>Tema anterior</h4>
  <p class="topless"><a href="learninspy.utils.html"
                        title="capítulo anterior">learninspy.utils package</a></p>
  <h4>Próximo tema</h4>
  <p class="topless"><a href="learninspy.utils.html"
                        title="próximo capítulo">learninspy.utils package</a></p>
  <h3>Esta página</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/learninspy.core.txt"
           rel="nofollow">Mostrar el código</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Búsqueda rápida</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Ir a" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Introduzca los términos de búsqueda o un nombre de módulo, clase o función.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navegación</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="learninspy.utils.html" title="learninspy.utils package"
             >siguiente</a></li>
        <li class="right" >
          <a href="learninspy.utils.html" title="learninspy.utils package"
             >anterior</a> |</li>
        <li><a href="index.html">documentación de Learninspy - 0.1.0</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2015, leferrad.
      Creado con <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>